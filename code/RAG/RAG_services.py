import yaml
import numpy as np
from transformers import AutoTokenizer
from QwenBasic import QwenRerankerNoKV
import time
import redis  
import json   

def softmax2(no_logit, yes_logit):
    m = max(no_logit, yes_logit)
    e_no = np.exp(no_logit - m)
    e_yes = np.exp(yes_logit - m)
    return float(e_yes / (e_no + e_yes))

def format_instruction(instruction, query, doc):
    if instruction is None:
        instruction = 'Given a web search query, retrieve relevant passages that answer the query'
    return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"

def broadcast_message(redis_conn, wav_path):
    """
    å¹¿æ’­æ¶ˆæ¯åˆ°æŒ‡å®šçš„Redisé¢‘é“ã€‚
    
    Args:
        redis_conn: Redisè¿æ¥å®ä¾‹ã€‚
        channel (str): ç›®æ ‡é¢‘é“åç§°ã€‚
        message (str): è¦å¹¿æ’­çš„æ¶ˆæ¯å†…å®¹ã€‚
    """
    command_data = {
        "action": "play",
        "path": wav_path,
        "key": 1
    }
    message = json.dumps(command_data)
    try:
        # 3. ä½¿ç”¨ PUBLISH å‘½ä»¤å°‡ JSON å­—ä¸²ç™¼é€åˆ°æŒ‡å®šçš„é »é“
        redis_conn.publish("audio:playcommand", message)
        print(f"âœ… æˆåŠŸå‘é€æŒ‡ä»¤åˆ°é »é“ 'audio:playcommand'")
        print(f"   è¨Šæ¯å…§å®¹: {message}")
    except Exception as e:
        print(f"âŒ ç™¼é€æŒ‡ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# 2. æ–°å¢ï¼šå°†Rerankerçš„æ ¸å¿ƒè®¡ç®—é€»è¾‘å°è£…æˆä¸€ä¸ªå‡½æ•°
def get_rerank_result(query, candidate_docs, qwen_model, tokenizer, prefix_tokens, suffix_tokens, yes_id, no_id):
    """
    æ¥æ”¶æŸ¥è¯¢å’Œæ–‡æ¡£åˆ—è¡¨ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£åŠå…¶åˆ†æ•°ã€‚
    """
    task = 'Given a web search query, retrieve relevant passages that answer the query'
    pairs = [format_instruction(task, query, doc) for doc in candidate_docs]

    scores = []
    for p in pairs:
        mid_ids = tokenizer.encode(p, add_special_tokens=False)
        token_ids = prefix_tokens + mid_ids + suffix_tokens
        if len(token_ids) > qwen_model.SEQLEN - 1:
            keep = qwen_model.SEQLEN - 1 - len(prefix_tokens) - len(suffix_tokens)
            mid_ids = mid_ids[:keep]
            token_ids = prefix_tokens + mid_ids + suffix_tokens

        logits = qwen_model.forward_once(token_ids)
        logit_no, logit_yes = float(logits[no_id]), float(logits[yes_id])
        prob_yes = softmax2(logit_no, logit_yes)
        scores.append(prob_yes)

    # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£
    if not scores:
        return None, 0.0

    best_score = max(scores)
    best_doc_index = scores.index(best_score)
    best_doc = candidate_docs[best_doc_index]

    print(f"Query: '{query}' çš„æœ€ä½³åŒ¹é…æ–‡æ¡£æ˜¯: '{best_doc}' (å¾—åˆ†: {best_score:.4f})")
    
    return best_doc, best_score

if __name__ == "__main__":
    # --- æ¨¡å‹å’ŒTokenizeråˆå§‹åŒ– (ä¿æŒä¸å˜) ---
    token_path = "/data/RAG_formal/models/qwen3-reranker/token_config_reranker"
    bmodel_path = "/data/RAG_formal/models/qwen3-reranker/qwen3-reranker-0.6b_w4bf16_seq512_bm1684x_1dev_20250818_123132.bmodel"
    dev_ids = "0"
    tokenizer = AutoTokenizer.from_pretrained(token_path, trust_remote_code=True)
    qwen = QwenRerankerNoKV(bmodel_path, dev_ids=dev_ids)
    
    prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
    suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)
    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)
    
    yes_ids = tokenizer.encode("yes", add_special_tokens=False)
    no_ids = tokenizer.encode("no", add_special_tokens=False)
    assert len(yes_ids) == 1 and len(no_ids) == 1, "Tokenizer must map 'yes' and 'no' to single tokens"
    yes_id, no_id = yes_ids[0], no_ids[0]

    # --- é™æ€çŸ¥è¯†åº“ (ä¿æŒä¸å˜) ---
    candidate_docs = [
        "æååŒ»ç”Ÿçš„æ¥è¯Šæ—¶é—´æ˜¯æ¯å‘¨ä¸€ã€å‘¨ä¸‰çš„ä¸Šåˆ9ç‚¹åˆ°12ç‚¹ï¼Œåœ°ç‚¹åœ¨é—¨è¯Šå¤§æ¥¼ä¸‰æ¥¼çš„çœ¼ç§‘è¯Šå®¤ã€‚",
        "å…³äºå¹´åº¦ä½“æ£€çš„é¢„çº¦ï¼Œè¯·åœ¨å·¥ä½œæ—¥ä¸‹åˆ2ç‚¹åˆ°5ç‚¹ä¹‹é—´ï¼Œè‡´ç”µ8857-1234è¿›è¡Œç”µè¯åŠç†ã€‚",
        "ç‹æ˜åŒ»ç”Ÿæ˜¯å¿ƒè„ç§‘ä¸“å®¶ï¼Œä»–çš„é—¨è¯Šæ—¶é—´æ˜¯æ¯å‘¨äºŒå…¨å¤©ï¼Œå’Œæ¯å‘¨äº”çš„ä¸‹åˆã€‚",
        "å‰å¾€å¸‚å›¾ä¹¦é¦†çš„608è·¯å¤§å·´è½¦ï¼Œæ¯å¤©æ—©ä¸Š7ç‚¹ä»å§‹å‘ç«™å‘è½¦ï¼Œæ¯éš”30åˆ†é’Ÿä¸€ç­ï¼Œæœ«ç­è½¦æ—¶é—´æ˜¯æ™šä¸Š8ç‚¹ã€‚",
        "æ³¨æ„ï¼šç”±äºå¸‚æ”¿é“è·¯æ–½å·¥ï¼Œä»æ˜å¤©èµ·ï¼Œ603è·¯å…¬äº¤è½¦çš„è·¯çº¿å°†ä¸´æ—¶æ”¹é“ï¼Œä¸å†ç»è¿‡ä¸­å¿ƒåŒ»é™¢ç«™ã€‚",
        "æˆ‘æŠŠå¤‡ç”¨é’¥åŒ™æ”¾åœ¨äº†å®¢å…è¿›é—¨å¤„ç„å…³çš„ç¬¬ä¸€ä¸ªæŠ½å±‰é‡Œï¼ŒæŒ¨ç€ä¸€æœ¬é»„è‰²çš„ç¬”è®°æœ¬ã€‚",
        "ä¸Šæ¬¡å»è¶…å¸‚è´­ä¹°äº†ç‰›å¥¶ã€é¢åŒ…å’Œé¸¡è›‹ï¼Œå…¶ä¸­ç‰›å¥¶çš„ä¿è´¨æœŸæ˜¯åˆ°8æœˆ26æ—¥ã€‚",
        "æˆ‘çš„å¥½å‹å°å¼ çš„ç”µè¯å·ç æ˜¯138-1234-5678ï¼Œä»–å®¶ä½åœ¨é˜³å…‰å°åŒºçš„Bæ ‹301ã€‚",
        "è®¾ç½®ä¸€ä¸ªæé†’ï¼šæ˜å¤©ä¸‹åˆ4ç‚¹éœ€è¦å»ç¤¾åŒºæœåŠ¡ä¸­å¿ƒé¢†å–æ–°çš„è¾…åŠ©è®¾å¤‡ã€‚"
    ]

    # 3. æ–°å¢ï¼šåˆå§‹åŒ–Rediså®¢æˆ·ç«¯
    try:
        redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        redis_client.ping()
        print("âœ… æˆåŠŸè¿æ¥åˆ° Redis æœåŠ¡å™¨ã€‚")
    except redis.exceptions.ConnectionError as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Redisï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡å™¨çŠ¶æ€: {e}")
        exit() # è¿æ¥å¤±è´¥åˆ™é€€å‡ºç¨‹åº

    # 4. æ–°å¢ï¼šåˆ›å»ºRedisè®¢é˜…å¯¹è±¡å¹¶è¿›å…¥ç›‘å¬å¾ªç¯
    pubsub = redis_client.pubsub()
    pubsub.subscribe('events:asr2rag')
    print("ğŸš€ RAGæœåŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬ 'events:asr2rag' é¢‘é“...")

    for message in pubsub.listen():
        if message['type'] != 'message':
            continue

        try:
            # è§£ææ”¶åˆ°çš„æ¶ˆæ¯
            data = json.loads(message['data'])
            instruction = data.get('instruction')
            query_text = data.get('text')

            if instruction == 'rag_query' and query_text:
                print(f"\nğŸ“© æ”¶åˆ°RAGè¯·æ±‚ï¼ŒæŸ¥è¯¢å†…å®¹: '{query_text}'")
                
                # è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
                best_doc, best_score = get_rerank_result(
                    query=query_text, 
                    candidate_docs=candidate_docs, 
                    qwen_model=qwen,
                    tokenizer=tokenizer,
                    prefix_tokens=prefix_tokens,
                    suffix_tokens=suffix_tokens,
                    yes_id=yes_id,
                    no_id=no_id
                )

                # å‡†å¤‡è¦å‘å¸ƒçš„æ–°æ¶ˆæ¯
                if best_doc and best_score > 0.1: # å¢åŠ ä¸€ä¸ªé˜ˆå€¼åˆ¤æ–­ï¼Œé¿å…è¿”å›ä¸ç›¸å…³çš„ç»“æœ
                    # å°†é—®é¢˜å’Œæ£€ç´¢åˆ°çš„èµ„æ–™æ‹¼æ¥ï¼Œé€ç»™Qwenè¿›è¡Œæœ€ç»ˆå›ç­”
                    final_qwen_input = f"å·²çŸ¥ä¿¡æ¯ï¼š'{best_doc}'ã€‚è¯·æ ¹æ®è¿™ä¸ªä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼š'{query_text}'"
                    
                    response_message = {
                        "instruction": "chatrag", # å°†æŒ‡ä»¤æ”¹ä¸º"chat"ï¼Œè®©Qwenä¸»æœåŠ¡è¿›è¡Œå¤„ç†
                        "text": final_qwen_input
                    }
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
                    response_message = {
                        "instruction": "chatrag",
                        "text": "æŠ±æ­‰ï¼Œå…³äºæ‚¨çš„é—®é¢˜ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                    }

                # å°†å¤„ç†ç»“æœå‘å¸ƒåˆ° 'events:asr_result'
                response_json = json.dumps(response_message, ensure_ascii=False)
                redis_client.publish('events:asr_result', response_json)
                print(f"ğŸ“¤ å·²å°†å¤„ç†ç»“æœå‘å¸ƒåˆ° 'events:asr_result': {response_json}")

            if instruction == 'rag_input' and query_text:
                candidate_docs.append(query_text)
                print(f"\nğŸ§  æ–°å¢è®°å¿†: '{query_text}'")
                print(f"   å½“å‰è®°å¿†åº“å…±æœ‰ {len(candidate_docs)} æ¡ä¿¡æ¯ã€‚")
                broadcast_message(redis_client,"/data/preaudio/003.wav")



        except json.JSONDecodeError:
            print(f"âš ï¸ æ— æ³•è§£ææ”¶åˆ°çš„æ¶ˆæ¯: {message['data']}")
        except Exception as e:
            print(f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")